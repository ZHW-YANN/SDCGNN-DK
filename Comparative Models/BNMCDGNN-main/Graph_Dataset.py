import glob
import os.path

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset


def load_graph_data(file_path):
    Total = {}
    for path in file_path:
        print('loading : {}'.format(path))
        try:
            graphs = np.load(path, allow_pickle=True)['graph_dict'].item()
        except UnicodeError:
            graphs = np.load(path, encoding='latin1', allow_pickle=True)['graph_dict'].item()
            graphs = {k.decode(): v for k, v in graphs.items()}
        Total = {**Total, **graphs}
    print('load successed, final volume : {}'.format(len(Total)))
    return Total


def collate_fn(batch):
    nodes = []
    edge_distance = []
    edge_targets = []
    edge_sources = []
    graph_indices = []
    node_counts = []
    targets = []
    combine_sets = []
    total_count = 0

    for i, (graph, target) in enumerate(batch):
        # Numbering for each batch
        nodes.append(graph.nodes)
        edge_distance.append(graph.distance)
        edge_sources.append(graph.edge_sources + total_count)  # source number of each edge
        edge_targets.append(graph.edge_targets + total_count)  # target number of each edge
        combine_sets.append(graph.combine_sets)
        node_counts.append(len(graph))
        targets.append(target)
        graph_indices += [i] * len(graph)
        total_count += len(graph)

    combine_sets = np.concatenate(combine_sets, axis=0)
    nodes = np.concatenate(nodes, axis=0)
    edge_distance = np.concatenate(edge_distance, axis=0)
    edge_sources = np.concatenate(edge_sources, axis=0)
    edge_targets = np.concatenate(edge_targets, axis=0)
    input = CDGNN_Input(nodes, edge_distance, edge_sources, edge_targets, graph_indices, node_counts, combine_sets)
    targets = torch.Tensor(targets)
    return input, targets


class Graph(object):
    def __init__(self, graph, cutoff, n_Gaussian):
        self.nodes, neighbors= graph
        nei = neighbors[0]
        distance = neighbors[1]
        vector = neighbors[2]
        n_nodes = len(self.nodes)
        self.nodes = np.array(self.nodes, dtype=np.float32)
        self.edge_sources = np.concatenate([[i] * len(nei[i]) for i in range(n_nodes)])
        self.edge_targets = np.concatenate(nei)
        edge_vector = np.array(vector, dtype=np.float32)
        self.edge_index = np.concatenate([range(len(nei[i])) for i in range(n_nodes)])
        self.vectorij = edge_vector[self.edge_sources, self.edge_index]
        edge_distance = np.array(distance, dtype=np.float32)
        self.distance = edge_distance[self.edge_sources, self.edge_index]
        combine_sets = []
        # gaussian radial
        N = n_Gaussian
        for n in range(1, N + 1):
            phi = Phi(self.distance, cutoff)
            G = gaussian(self.distance, miuk(n, N, cutoff), betak(N, cutoff))
            combine_sets.append(phi * G)
        self.combine_sets = np.array(combine_sets, dtype=np.float32).transpose()

    def __len__(self):
        return len(self.nodes)


class GraphDataset(Dataset):
    def __init__(self, path, filename, database, target_name, cutoff, n_Gaussian):
        super(GraphDataset, self).__init__()

        target_path = os.path.join(path, "targets_" + database + ".csv")

        if target_name == 'Ea' and database == 'ICSD':
            target_path = os.path.join(path, "targets_Ea.csv")
        elif target_name == 'formation_energy_per_atom' and database == 'MP':
            target_path = os.path.join(path, "targets_" + database + '_Ef' + ".csv")

        df = pd.read_csv(target_path).dropna(axis=0, how='any')

        graph_data_path = sorted(glob.glob(os.path.join(path, 'npz/' + filename + '*.npz')))
        print('The number of files = {}'.format(len(graph_data_path)))
        self.graph_data = load_graph_data(graph_data_path)
        graphs = self.graph_data.keys()

        self.graph_names = df.loc[df['id'].isin(graphs)].id.values.tolist()
        self.targets = np.array(df.loc[df['id'].isin(graphs)][target_name].values.tolist())
        print('the number of valid targets = {}'.format(len(self.targets)))
        print('start to constructe Graph')
        graph_data = []
        for i, name in enumerate(self.graph_names):
            graph_data.append(Graph(self.graph_data[name], cutoff, n_Gaussian))
            if i % 500 == 0 and i > 0:
                print('{} graphs constructed'.format(i))
        print('finish constructe the graph')
        self.graph_data = graph_data

        assert (len(self.graph_data) == len(self.targets))
        print('The number of valid graphs = {}'.format(len(self.targets)))

    def __getitem__(self, index):
        return self.graph_data[index], self.targets[index]

    def __len__(self):
        return len(self.graph_names)


# 构建torch的输入张量
class CDGNN_Input(object):
    def __init__(self, nodes, edge_distance, edge_sources, edge_targets, graph_indices, node_counts, combine_sets):
        self.nodes = torch.Tensor(nodes)
        self.edge_distance = torch.Tensor(edge_distance)
        self.edge_sources = torch.LongTensor(edge_sources)
        self.edge_targets = torch.LongTensor(edge_targets)
        self.graph_indices = torch.LongTensor(graph_indices)
        self.node_counts = torch.Tensor(node_counts)
        self.combine_sets = torch.Tensor(combine_sets)

    def __len__(self):
        return self.nodes.size(0)



def a_RBF(n, d, cutoff):
    return np.sqrt(2 / cutoff) * np.sin(n * np.pi * d / cutoff) / d



def Phi(r, cutoff):
    return 1 - 6 * (r / cutoff) ** 5 + 15 * (r / cutoff) ** 4 - 10 * (r / cutoff) ** 3


def gaussian(r, miuk, betak):
    return np.exp(-betak * (np.exp(-r) - miuk) ** 2)


def miuk(n, K, cutoff):
    # n=[1,K]
    return np.exp(-cutoff) + (1 - np.exp(-cutoff)) / K * n


def betak(K, cutoff):
    return (2 / K * (1 - np.exp(-cutoff))) ** (-2)
